{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779faf36-2612-4e9c-8542-12a198c2cd44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Unknown dataset name: incrementaltestCIFAR100-0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "---------------------\n",
      "Train/Test accuracy: 62.980000000000004 46.0\n",
      "confidence-based MI attack AUC: 0.5576554584726947\n",
      "Gap attack AUC: 0.5944026439940259\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "!python membership_inference_attack.py -d incrementaltestCIFAR100-0 -em FLglobalmodel10A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b4d27c-0675-4234-8913-4a6230862216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Unknown dataset name: incrementaltestCIFAR100-1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "---------------------\n",
      "Train/Test accuracy: 73.31 53.42\n",
      "confidence-based MI attack AUC: 0.5521315101364359\n",
      "Gap attack AUC: 0.5975208299476891\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "!python membership_inference_attack.py -d incrementaltestCIFAR100-1 -em FLglobalmodel10B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941b6e31-7440-4f25-9f9d-4f202576777b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Unknown dataset name: incrementaltestCIFAR100-2\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "---------------------\n",
      "Train/Test accuracy: 80.07000000000001 56.730000000000004\n",
      "confidence-based MI attack AUC: 0.5526898677891742\n",
      "Gap attack AUC: 0.6157116130774754\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "!python membership_inference_attack.py -d incrementaltestCIFAR100-2 -em FLglobalmodel10C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100d29e8-0046-47e4-81c5-1104ef167f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Unknown dataset name: incrementaltestCIFAR100-3\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "---------------------\n",
      "Train/Test accuracy: 80.91 58.88\n",
      "confidence-based MI attack AUC: 0.5559532145860908\n",
      "Gap attack AUC: 0.6099314056548437\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "!python membership_inference_attack.py -d incrementaltestCIFAR100-3 -em FLglobalmodel10D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639eb79c-b143-4325-8728-d14893bd859f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Unknown dataset name: incrementaltestCIFAR100-4\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "---------------------\n",
      "Train/Test accuracy: 8.27 7.05\n",
      "confidence-based MI attack AUC: 0.5115648525505784\n",
      "Gap attack AUC: 0.5080589593765822\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "!python membership_inference_attack.py -d incrementaltestCIFAR100-4 -em FLglobalmodel10E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e80c6f-28e3-44f3-af2b-3b6f4432d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\n",
      "Training efficientnet with incrementalCIFAR100 in cpu using PyTorch 1.13.1+cu117 and NVFlare 2.2.6\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "/home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "Training centralized model for 50 epochs\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "patience: 100%|████████████████████████████████| 5/5 [00:00<00:00, 40800.62it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   2%|▋                               | 1/50 [01:50<1:30:28, 110.78s/it]\u001b[A\n",
      "\n",
      "patience: 100%|███████████████████████████████████| 5/5 [01:50<00:00, 22.16s/it]\u001b[A\u001b[A\n",
      "Epoch: 1:   2%|▌                              | 1/50 [01:50<1:30:28, 110.78s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0699, loss: 0.0640, train_acc 0.3232, acc: 0.3995, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 1:   4%|█▏                             | 2/50 [03:39<1:27:48, 109.77s/it]\u001b[A\n",
      "Epoch: 2:   4%|█▏                             | 2/50 [03:39<1:27:48, 109.77s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0529, loss: 0.0578, train_acc 0.4736, acc: 0.4796, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 2:   6%|█▊                             | 3/50 [05:29<1:26:00, 109.80s/it]\u001b[A\n",
      "Epoch: 3:   6%|█▊                             | 3/50 [05:29<1:26:00, 109.80s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0450, loss: 0.0550, train_acc 0.5483, acc: 0.5044, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 3:   8%|██▍                            | 4/50 [07:18<1:23:52, 109.40s/it]\u001b[A\n",
      "Epoch: 4:   8%|██▍                            | 4/50 [07:18<1:23:52, 109.40s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0387, loss: 0.0618, train_acc 0.6055, acc: 0.5276, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 4:  10%|███                            | 5/50 [09:07<1:22:04, 109.43s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0387, loss: 0.0618, train_acc 0.6055, acc: 0.5276, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 5:  10%|███                            | 5/50 [09:07<1:22:04, 109.43s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0336, loss: 0.0485, train_acc 0.6577, acc: 0.5556, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 5:  12%|███▋                           | 6/50 [10:56<1:20:01, 109.13s/it]\u001b[A\n",
      "Epoch: 6:  12%|███▋                           | 6/50 [10:56<1:20:01, 109.13s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0295, loss: 0.0547, train_acc 0.6973, acc: 0.5372, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 6:  14%|████▎                          | 7/50 [12:44<1:18:01, 108.87s/it]\u001b[A\n",
      "Epoch: 7:  14%|████▎                          | 7/50 [12:44<1:18:01, 108.87s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0278, loss: 0.0528, train_acc 0.7186, acc: 0.5420, Patience: :  60\u001b[A\u001b[A\n",
      "Epoch: 7:  16%|████▉                          | 8/50 [14:33<1:16:08, 108.79s/it]\u001b[A\n",
      "Epoch: 8:  16%|████▉                          | 8/50 [14:33<1:16:08, 108.79s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0239, loss: 0.0542, train_acc 0.7539, acc: 0.5444, Patience: :  40\u001b[A\u001b[A\n",
      "Epoch: 8:  18%|█████▌                         | 9/50 [16:22<1:14:17, 108.72s/it]\u001b[A\n",
      "Epoch: 9:  18%|█████▌                         | 9/50 [16:22<1:14:17, 108.72s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0207, loss: 0.0577, train_acc 0.7900, acc: 0.5252, Patience: :  20\u001b[A\u001b[A\n",
      "Epoch: 9:  20%|██████                        | 10/50 [18:11<1:12:33, 108.83s/it]\u001b[A\n",
      "Epoch: 10:  20%|█████▊                       | 10/50 [18:11<1:12:33, 108.83s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0187, loss: 0.0554, train_acc 0.8129, acc: 0.5564, Patience: :   0\u001b[A\u001b[A\n",
      "Epoch: 10:  22%|██████▍                      | 11/50 [20:41<1:18:55, 121.43s/it]\u001b[A\n",
      "\n",
      "Early stopped at epoch 11, train_loss: 0.0187, loss: 0.0500, train_acc 0.8129, a\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[AFinal validation set performance:\n",
      "\tloss 0.049995085676908496\n",
      "\taccuracy 0.5307\n",
      "Final test set performance:\n",
      "\tloss 0.049995085676908496\n",
      "\taccuracy 0.5307\n",
      "\n",
      "Saved model to ./saved_models/CentralizedefficientnetincrementalCIFAR1000.pt\n",
      "\n",
      "Training efficientnet with incrementalCIFAR100 in cpu using PyTorch 1.13.1+cu117 and NVFlare 2.2.6\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Training centralized model for 50 epochs\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "patience: 100%|████████████████████████████████| 5/5 [00:00<00:00, 39053.11it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   2%|▋                               | 1/50 [01:51<1:31:17, 111.78s/it]\u001b[A\n",
      "\n",
      "patience: 100%|███████████████████████████████████| 5/5 [01:51<00:00, 22.36s/it]\u001b[A\u001b[A\n",
      "Epoch: 1:   2%|▌                              | 1/50 [01:51<1:31:17, 111.78s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0688, loss: 0.0608, train_acc 0.3336, acc: 0.4572, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 1:   4%|█▏                             | 2/50 [03:42<1:28:42, 110.89s/it]\u001b[A\n",
      "Epoch: 2:   4%|█▏                             | 2/50 [03:42<1:28:42, 110.89s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0515, loss: 0.0628, train_acc 0.4896, acc: 0.5004, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 2:   6%|█▊                             | 3/50 [05:33<1:27:04, 111.16s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0515, loss: 0.0628, train_acc 0.4896, acc: 0.5004, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 3:   6%|█▊                             | 3/50 [05:33<1:27:04, 111.16s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0433, loss: 0.0482, train_acc 0.5618, acc: 0.5236, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 3:   8%|██▍                            | 4/50 [07:23<1:24:57, 110.82s/it]\u001b[A\n",
      "Epoch: 4:   8%|██▍                            | 4/50 [07:23<1:24:57, 110.82s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0372, loss: 0.0473, train_acc 0.6226, acc: 0.5540, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 4:  10%|███                            | 5/50 [09:12<1:22:38, 110.18s/it]\u001b[A\n",
      "Epoch: 5:  10%|███                            | 5/50 [09:12<1:22:38, 110.18s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0333, loss: 0.0516, train_acc 0.6598, acc: 0.5412, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 5:  12%|███▋                           | 6/50 [11:04<1:21:16, 110.83s/it]\u001b[A\n",
      "Epoch: 6:  12%|███▋                           | 6/50 [11:04<1:21:16, 110.83s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0295, loss: 0.0508, train_acc 0.6995, acc: 0.5492, Patience: :  60\u001b[A\u001b[A\n",
      "Epoch: 6:  14%|████▎                          | 7/50 [12:55<1:19:19, 110.68s/it]\u001b[A\n",
      "Epoch: 7:  14%|████▎                          | 7/50 [12:55<1:19:19, 110.68s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0252, loss: 0.0497, train_acc 0.7426, acc: 0.5781, Patience: :  40\u001b[A\u001b[A\n",
      "Epoch: 7:  16%|████▉                          | 8/50 [14:45<1:17:15, 110.37s/it]\u001b[A\n",
      "Epoch: 8:  16%|████▉                          | 8/50 [14:45<1:17:15, 110.37s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0232, loss: 0.0513, train_acc 0.7622, acc: 0.5637, Patience: :  20\u001b[A\u001b[A\n",
      "Epoch: 8:  18%|█████▌                         | 9/50 [16:35<1:15:22, 110.30s/it]\u001b[A\n",
      "Epoch: 9:  18%|█████▌                         | 9/50 [16:35<1:15:22, 110.30s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0213, loss: 0.0528, train_acc 0.7825, acc: 0.5733, Patience: :   0\u001b[A\u001b[A\n",
      "Epoch: 9:  20%|██████                        | 10/50 [19:02<1:21:12, 121.81s/it]\u001b[A\n",
      "\n",
      "Early stopped at epoch 10, train_loss: 0.0213, loss: 0.0463, train_acc 0.7825, a\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[AFinal validation set performance:\n",
      "\tloss 0.04630093486189842\n",
      "\taccuracy 0.5421\n",
      "Final test set performance:\n",
      "\tloss 0.04630093486189842\n",
      "\taccuracy 0.5421\n",
      "\n",
      "Saved model to ./saved_models/CentralizedefficientnetincrementalCIFAR1001.pt\n",
      "\n",
      "Training efficientnet with incrementalCIFAR100 in cpu using PyTorch 1.13.1+cu117 and NVFlare 2.2.6\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Training centralized model for 50 epochs\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "patience: 100%|████████████████████████████████| 5/5 [00:00<00:00, 41859.32it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   2%|▋                               | 1/50 [01:50<1:29:57, 110.16s/it]\u001b[A\n",
      "\n",
      "patience: 100%|███████████████████████████████████| 5/5 [01:50<00:00, 22.03s/it]\u001b[A\u001b[A\n",
      "Epoch: 1:   2%|▌                              | 1/50 [01:50<1:29:57, 110.16s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0702, loss: 0.0847, train_acc 0.3199, acc: 0.3827, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 1:   4%|█▏                             | 2/50 [03:41<1:28:29, 110.62s/it]\u001b[A\n",
      "Epoch: 2:   4%|█▏                             | 2/50 [03:41<1:28:29, 110.62s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0528, loss: 0.0551, train_acc 0.4716, acc: 0.4956, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 2:   6%|█▊                             | 3/50 [05:31<1:26:42, 110.69s/it]\u001b[A\n",
      "Epoch: 3:   6%|█▊                             | 3/50 [05:31<1:26:42, 110.69s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0443, loss: 0.0482, train_acc 0.5506, acc: 0.5508, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 3:   8%|██▍                            | 4/50 [07:21<1:24:37, 110.38s/it]\u001b[A\n",
      "Epoch: 4:   8%|██▍                            | 4/50 [07:21<1:24:37, 110.38s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0393, loss: 0.0491, train_acc 0.6027, acc: 0.5620, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 4:  10%|███                            | 5/50 [09:13<1:23:05, 110.78s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0393, loss: 0.0491, train_acc 0.6027, acc: 0.5620, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 5:  10%|███                            | 5/50 [09:13<1:23:05, 110.78s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0340, loss: 0.0473, train_acc 0.6513, acc: 0.5556, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 5:  12%|███▋                           | 6/50 [11:03<1:21:12, 110.74s/it]\u001b[A\n",
      "Epoch: 6:  12%|███▋                           | 6/50 [11:03<1:21:12, 110.74s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0303, loss: 0.0526, train_acc 0.6874, acc: 0.5572, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 6:  14%|████▎                          | 7/50 [12:54<1:19:25, 110.84s/it]\u001b[A\n",
      "Epoch: 7:  14%|████▎                          | 7/50 [12:54<1:19:25, 110.84s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0276, loss: 0.0485, train_acc 0.7126, acc: 0.5588, Patience: :  60\u001b[A\u001b[A\n",
      "Epoch: 7:  16%|████▉                          | 8/50 [14:45<1:17:27, 110.65s/it]\u001b[A\n",
      "Epoch: 8:  16%|████▉                          | 8/50 [14:45<1:17:27, 110.65s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0242, loss: 0.0530, train_acc 0.7506, acc: 0.5669, Patience: :  40\u001b[A\u001b[A\n",
      "Epoch: 8:  18%|█████▌                         | 9/50 [16:34<1:15:24, 110.34s/it]\u001b[A\n",
      "Epoch: 9:  18%|█████▌                         | 9/50 [16:34<1:15:24, 110.34s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0209, loss: 0.0524, train_acc 0.7837, acc: 0.5677, Patience: :  20\u001b[A\u001b[A\n",
      "Epoch: 9:  20%|██████                        | 10/50 [18:24<1:13:29, 110.24s/it]\u001b[A\n",
      "Epoch: 10:  20%|█████▊                       | 10/50 [18:24<1:13:29, 110.24s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0199, loss: 0.0552, train_acc 0.7918, acc: 0.5645, Patience: :   0\u001b[A\u001b[A\n",
      "Epoch: 10:  22%|██████▍                      | 11/50 [20:53<1:19:15, 121.94s/it]\u001b[A\n",
      "\n",
      "Early stopped at epoch 11, train_loss: 0.0199, loss: 0.0502, train_acc 0.7918, a\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[AFinal validation set performance:\n",
      "\tloss 0.050223165278434755\n",
      "\taccuracy 0.5297\n",
      "Final test set performance:\n",
      "\tloss 0.050223165278434755\n",
      "\taccuracy 0.5297\n",
      "\n",
      "Saved model to ./saved_models/CentralizedefficientnetincrementalCIFAR1002.pt\n",
      "\n",
      "Training efficientnet with incrementalCIFAR100 in cpu using PyTorch 1.13.1+cu117 and NVFlare 2.2.6\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using cache found in /home/saham001/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Training centralized model for 50 epochs\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "patience: 100%|████████████████████████████████| 5/5 [00:00<00:00, 33130.36it/s]\u001b[A\u001b[A\n",
      "Epoch 1:   2%|▋                               | 1/50 [01:50<1:30:19, 110.59s/it]\u001b[A\n",
      "\n",
      "patience: 100%|███████████████████████████████████| 5/5 [01:50<00:00, 22.12s/it]\u001b[A\u001b[A\n",
      "Epoch: 1:   2%|▌                              | 1/50 [01:50<1:30:19, 110.59s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0685, loss: 0.0709, train_acc 0.3310, acc: 0.3773, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 1:   4%|█▏                             | 2/50 [03:41<1:28:31, 110.67s/it]\u001b[A\n",
      "Epoch: 2:   4%|█▏                             | 2/50 [03:41<1:28:31, 110.67s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0591, loss: 0.0584, train_acc 0.4172, acc: 0.4293, Patience: : 100\u001b[A\u001b[A\n",
      "Epoch: 2:   6%|█▊                             | 3/50 [05:31<1:26:26, 110.35s/it]\u001b[A\n",
      "Epoch: 3:   6%|█▊                             | 3/50 [05:31<1:26:26, 110.35s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0666, loss: 0.0677, train_acc 0.3436, acc: 0.3317, Patience: :  80\u001b[A\u001b[A\n",
      "Epoch: 3:   8%|██▍                            | 4/50 [07:21<1:24:40, 110.44s/it]\u001b[A\n",
      "Epoch: 4:   8%|██▍                            | 4/50 [07:21<1:24:40, 110.44s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0595, loss: 0.0623, train_acc 0.4048, acc: 0.4141, Patience: :  60\u001b[A\u001b[A\n",
      "Epoch: 4:  10%|███                            | 5/50 [09:12<1:22:46, 110.36s/it]\u001b[A\n",
      "Epoch: 5:  10%|███                            | 5/50 [09:12<1:22:46, 110.36s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0560, loss: 0.0598, train_acc 0.4378, acc: 0.4277, Patience: :  40\u001b[A\u001b[A\n",
      "Epoch: 5:  12%|███▋                           | 6/50 [11:02<1:20:58, 110.42s/it]\u001b[A\n",
      "Epoch: 6:  12%|███▋                           | 6/50 [11:02<1:20:58, 110.42s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0553, loss: 0.0625, train_acc 0.4475, acc: 0.3973, Patience: :  20\u001b[A\u001b[A\n",
      "Epoch: 6:  14%|████▎                          | 7/50 [12:53<1:19:08, 110.42s/it]\u001b[A\n",
      "Epoch: 7:  14%|████▎                          | 7/50 [12:53<1:19:08, 110.42s/it]\u001b[A\n",
      "\n",
      "train_loss: 0.0754, loss: 0.0732, train_acc 0.2634, acc: 0.2998, Patience: :   0\u001b[A\u001b[A\n",
      "Epoch: 7:  16%|████▉                          | 8/50 [15:22<1:25:54, 122.72s/it]\u001b[A\n",
      "\n",
      "Early stopped at epoch 8, train_loss: 0.0754, loss: 0.0579, train_acc 0.2634, ac\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[AFinal validation set performance:\n",
      "\tloss 0.057931787478923795\n",
      "\taccuracy 0.4293\n",
      "Final test set performance:\n",
      "\tloss 0.057931787478923795\n",
      "\taccuracy 0.4293\n",
      "\n",
      "Saved model to ./saved_models/CentralizedefficientnetincrementalCIFAR1003.pt\n"
     ]
    }
   ],
   "source": [
    "!python centralized.py -d incrementalCIFAR100_4 -e 50 -m efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bdfa03-827c-46ea-90e0-1e1d5a8a84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.datasets import load_partitioned_datasets as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "39bbb6da-9f6f-46e6-b0fa-edd8b6ae25b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"incrementalCIFAR100=ABCD_3_0\"\n",
    "num_clients = 2\n",
    "data_path=\"~/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0162c6da-61f7-46f3-a25c-209ee9608a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_parameters_list = dataset_name.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f4f07061-5065-40b7-a940-4c0a3596ab04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(dataset_parameters_list) == 2:\n",
    "    dataset_name, data_index = dataset_parameters_list\n",
    "    data_index = int(data_index)\n",
    "    split=None\n",
    "else:\n",
    "    dataset_name, split, data_index = dataset_parameters_list\n",
    "    data_index= int(data_index)\n",
    "    split=int(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4f172b25-9175-4758-98cd-fca7e9d2c26d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('incrementalCIFAR100=ABCD', 3, 0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name, split, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "689dc0e7-6ec0-4e5f-8522-9122b2202440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  \r"
     ]
    }
   ],
   "source": [
    "[trainloader, valloaders, testloader, _ ], num_channels, num_classes = d(num_clients=num_clients, dataset_name=dataset_name, \n",
    "                                                                                                         data_path=data_path, batch_size=32,split=split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94ac6d8c-94f0-4b14-a905-c152fdd8dc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = trainloader[data_index]\n",
    "valloader = valloaders[data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d462031f-178a-460a-9f72-2d034a3aa913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57d6255f-74ee-4707-a6ae-c79d24c7b3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d022e-1c22-426a-b4bc-5266fbb9c706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e92ffc-3088-4eee-b943-63f882c4180c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
