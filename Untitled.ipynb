{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d588ae8-1c00-46f6-9413-924352d227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST, CIFAR100, SVHN, FashionMNIST\n",
    "from torch.utils.data import  Dataset, DataLoader, ConcatDataset, Subset, TensorDataset, random_split\n",
    "import pdb,traceback\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from utilities.lib import blockPrinting\n",
    "from utilities.cifar100_fine_coarse_labels import remapping\n",
    "import pdb,traceback\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_remapping(choice='ABCD', n=20):\n",
    "    # Generate initial mapping based on consecutive division\n",
    "    \n",
    "    initial_mapping = [list(range(i, min(i + n // len(choice), n))) for i in range(0, n, n // len(choice))]\n",
    "    \n",
    "    # Create a dictionary to hold the remappings\n",
    "    remapping = [initial_mapping[ord(c) - ord('A')] for c in choice]\n",
    "    \n",
    "    return remapping\n",
    "\n",
    "class IncrementalDatasetWraper():\n",
    "    def __init__(self, dataset_name = 'incremental_SVHN', data_path=\"~/datasets\", audit_mode = False, addetive_train = False):\n",
    "        self.name = dataset_name\n",
    "        self.audit_mode = audit_mode\n",
    "        self.splits = self._load_datasets(dataset_name)\n",
    "        if addetive_train:\n",
    "            self.splits = implement_addetive_dataset(self.splits, additive_train =True)\n",
    "\n",
    "    def _load_datasets(self, dataset_name, data_path=\"~/dataset\"):\n",
    "        try: \n",
    "            dataset_name, remapping = dataset_name.split('=')\n",
    "        except: \n",
    "            remapping = 'ABCD'\n",
    "        \n",
    "        if dataset_name == 'incrementalSVHN':\n",
    "            remapping = get_remapping(remapping, n=10)\n",
    "            data_splits = load_incremental_local_SVHN(data_path, remapping=remapping, uniform_test = True)\n",
    "        elif dataset_name == 'incrementaltestSVHN':\n",
    "            remapping = get_remapping(remapping, n=10)\n",
    "            data_splits = load_incremental_local_SVHN(data_path, remapping=remapping, uniform_test = False)\n",
    "        elif dataset_name == 'incrementalCIFAR100':\n",
    "            remapping = get_remapping(remapping, n=20)\n",
    "            data_splits = load_incremental_CIFAR20(data_path, remapping=remapping, uniform_test = True)\n",
    "        elif dataset_name == 'incrementaltestCIFAR100':\n",
    "            remapping = get_remapping(remapping, n=20)\n",
    "            data_splits = load_incremental_CIFAR20(data_path, remapping=remapping, uniform_test = False)\n",
    "        elif dataset_name == 'incrementalCIFAR10' or dataset_name == 'incrementalMNIST' or dataset_name == 'incrementalFashionMNIST':\n",
    "            remapping = get_remapping(remapping, n=10)\n",
    "            data_splits = load_incremental(dataset_name, data_path, remapping=remapping, uniform_test = True)\n",
    "        elif dataset_name == 'incrementaltestCIFAR10' or dataset_name == 'incrementaltestMNIST' or dataset_name == 'incrementaltestFashionMNIST':\n",
    "            remapping = get_remapping(remapping, n=10)\n",
    "            data_splits = load_incremental(dataset_name, data_path, remapping=remapping, uniform_test = False)\n",
    "        else:\n",
    "            print(f'Unknown dataset name: {dataset_name}')\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # for index, (train_subset, test_subset, num_channels, num_classes) in enumerate(data_splits):\n",
    "        #     modified_trainset, modified_testset = remap_dataset(self.audit_mode, train_subset, test_subset)\n",
    "        #     updated_split = (modified_trainset, modified_testset, num_channels, num_classes)\n",
    "        #     data_splits[index] = updated_split\n",
    "\n",
    "        return data_splits\n",
    "        \n",
    "    def select_split(self, split):\n",
    "        self.trainset, self.testset, self.num_channels, self.num_classes = self.splits[split]\n",
    "        self.data_split = [self.trainset, self.testset, self.num_channels, self.num_classes]\n",
    "\n",
    "def load_datasets_by_name(dataset_name, data_path ):\n",
    "        if dataset_name == 'CIFAR10':\n",
    "            return load_CIFAR10(data_path)\n",
    "        elif dataset_name == 'CIFAR100':\n",
    "            return load_CIFAR100(data_path)\n",
    "        elif dataset_name == 'MNIST':\n",
    "            return load_MNIST(data_path)\n",
    "        elif dataset_name == 'FashionMNIST':\n",
    "            return load_FashionMNIST(data_path)\n",
    "        elif dataset_name == \"SVHN\":\n",
    "            return load_SVHN(data_path)\n",
    "        else:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print(f'Unknown dataset name: {dataset_name}')\n",
    "            raise NotImplementedError\n",
    "\n",
    "class DatasetWrapper():\n",
    "    def __init__(self, dataset_name = 'CIFAR10', data_path=\"~/dataset\"):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.name = dataset_name\n",
    "        self.trainset, self.testset, self.num_channels, self.num_classes = self._load_datasets(dataset_name)\n",
    "\n",
    "\n",
    "    # @blockPrinting  \n",
    "    def _load_datasets(self, dataset_name):\n",
    "        return load_datasets_by_name(dataset_name, self.data_path )\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def load_CIFAR10(data_path=\"~/dataset\"):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(root=data_path, train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    num_channels=3\n",
    "    num_classes=10\n",
    "\n",
    "\n",
    "    #full_dataset = ConcatDataset([train_dataset,test_dataset])\n",
    "\n",
    "    #train_size = int(len(full_dataset)*train_percent)\n",
    "    #test_size = len(full_dataset) - train_size\n",
    "\n",
    "    #trainset, testset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return trainset, testset, num_channels, num_classes\n",
    "\n",
    "def load_CIFAR100(data_path=\"~/dataset\"):\n",
    "    # Download and transform CIFAR-100 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR100(root=data_path, train=True, download=True, transform=transform)\n",
    "    testset = CIFAR100(root=data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    num_channels = 3\n",
    "    num_classes = 100\n",
    "\n",
    "    return trainset, testset, num_channels, num_classes\n",
    "\n",
    "def load_SVHN(data_path=\"~/dataset\"):\n",
    "    # Download and transform SVHN (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = SVHN(root=data_path, split=\"train\", download=True, transform=transform)\n",
    "    testset = SVHN(root=data_path, split=\"test\", download=True, transform=transform)\n",
    "\n",
    "    num_channels=3\n",
    "    num_classes = 10\n",
    "\n",
    "    return trainset, testset, num_channels, num_classes\n",
    "\n",
    "def load_MNIST(data_path=\"~/dataset\"):\n",
    "    # Download and transform MNIST (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=3), #expand to 3 channels\n",
    "        transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = MNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "    testset = MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "    num_channels = 3\n",
    "    #num_channels = 1\n",
    "    num_classes = 10\n",
    "\n",
    "    return trainset, testset, num_channels, num_classes\n",
    "\n",
    "\n",
    "def load_FashionMNIST(data_path=\"~/dataset\"):\n",
    "    # Download and transform FashionMNIST (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(num_output_channels=3),  # Expand to 3 channels\n",
    "            transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    trainset = FashionMNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "    testset = FashionMNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    num_channels = 3  # Set to 3 channels\n",
    "    num_classes = 10\n",
    "\n",
    "    return trainset, testset, num_channels, num_classes\n",
    "\n",
    "def load_incremental_local_SVHN(data_path, remapping, uniform_test):\n",
    "    data_path = os.path.expanduser(data_path)\n",
    "    splits_paths = [\n",
    "        os.path.join(data_path, 'SVHN','extra_A'),\n",
    "        os.path.join(data_path, 'SVHN','extra_B'),\n",
    "        os.path.join(data_path, 'SVHN','extra_C'),\n",
    "        os.path.join(data_path, 'SVHN','train_cropped_images'),\n",
    "        os.path.join(data_path, 'SVHN','test_cropped_images')\n",
    "    ]\n",
    "\n",
    "    return load_incremental_local_dataset(splits_paths, remapping, uniform_test)\n",
    "\n",
    "def load_incremental_local_dataset(splits_paths, remapping, uniform_test = True):\n",
    "    data_splits = []\n",
    "    print('Loading custom incremental dataset...')\n",
    "    for directory in tqdm(splits_paths, leave=False):\n",
    "        train_dataset, test_dataset, num_channels, num_classes = load_custom_image_dataset(directory, test_size=0.4)\n",
    "        data_splits.append((train_dataset, test_dataset, num_channels, num_classes))\n",
    "\n",
    "    # combine the train test and the extras into a single monolithic dataset\n",
    "    data_splits = combine_subsets(data_splits, [list(range(len(data_splits)))])\n",
    "\n",
    "    #now seperate the monolithic dataset into 10 subsets [0-9]\n",
    "\n",
    "    \n",
    "    train_subsets = split_dataset_into_subsets(data_splits[0][0], num_classes)\n",
    "    test_subsets = split_dataset_into_subsets(data_splits[0][1], num_classes)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Combine the train and test subsets along with num_channels and num_classes into a list of tuples\n",
    "    data_splits = [(train_subsets[i], test_subsets[i], num_channels, num_classes) for i in range(len(train_subsets))]\n",
    "    # mix the  classes in the dataset together\n",
    "    data_splits = mix_subsets(data_splits)\n",
    "\n",
    "    # combine the subsets with remapping\n",
    "    data_splits = combine_subsets(data_splits, remapping)\n",
    "\n",
    "\n",
    "    if uniform_test:\n",
    "        data_splits = implement_combined_uniform_test(data_splits)\n",
    "    else:\n",
    "        data_splits = implement_addetive_dataset(data_splits)\n",
    "\n",
    "    return data_splits\n",
    "\n",
    "def load_custom_image_dataset(directory, test_size=0.4):\n",
    "    images = []\n",
    "    labels = []\n",
    "    try:\n",
    "        train_dataset, test_dataset, num_channels, num_classes =  load_pickle(directory+'.pkl')\n",
    "    except:\n",
    "        print(f'\\nPresaved dataset not found, Loading custom dataset from {directory}')\n",
    "        for label in tqdm(os.listdir(directory), leave=False):\n",
    "            label_dir = os.path.join(directory, label)\n",
    "            for img_file in tqdm(os.listdir(label_dir), leave=False):\n",
    "                img_path = os.path.join(label_dir, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (32, 32))  # Resize image to a fixed size\n",
    "                images.append(img)\n",
    "                labels.append(int(label))\n",
    "    \n",
    "        # Transform the dataset\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        dataset = [(transform(img), label) for img, label in zip(images, labels)]\n",
    "    \n",
    "        # Calculate the sizes of the train and test sets based on the test_size ratio\n",
    "        test_size = int(len(dataset) * test_size)\n",
    "        train_size = len(dataset) - test_size\n",
    "    \n",
    "        # Split the dataset into training and test sets\n",
    "        torch.manual_seed(42)\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size]) # type: ignore\n",
    "    \n",
    "        num_channels = 3\n",
    "        num_classes = len(np.unique(labels))\n",
    "\n",
    "        try:\n",
    "            save_pickle( (train_dataset, test_dataset, num_channels, num_classes), directory+'.pkl')\n",
    "        except Exception as e:\n",
    "            print('Error saving dataset:', e)\n",
    "\n",
    "\n",
    "   \n",
    "    return train_dataset, test_dataset, num_channels, num_classes\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_incremental(dataset_name, data_path, remapping= None, uniform_test = False):\n",
    "    if 'test' in dataset_name:\n",
    "        datasetname = dataset_name.replace('incrementaltest','')\n",
    "    else:\n",
    "        datasetname = dataset_name.replace('incremental','')\n",
    "        \n",
    "    trainset, testset, num_channels, num_classes = load_datasets_by_name(datasetname, data_path )\n",
    "\n",
    "    \n",
    "    #now seperate the monolithic dataset into 10 subsets [0-9]\n",
    "    train_subsets = split_dataset_into_subsets(trainset, num_classes)\n",
    "    test_subsets = split_dataset_into_subsets(testset, num_classes)\n",
    "    \n",
    "\n",
    "    # Combine the train and test subsets along with num_channels and num_classes into a list of tuples\n",
    "    data_splits = [(train_subsets[i], test_subsets[i], num_channels, num_classes) for i in range(len(train_subsets))]\n",
    "    # mix the  classes in the dataset together\n",
    "    data_splits = mix_subsets(data_splits)\n",
    "\n",
    "    # combine the subsets with remapping\n",
    "    data_splits = combine_subsets(data_splits, remapping)\n",
    "\n",
    "\n",
    "    if uniform_test:\n",
    "        data_splits = implement_combined_uniform_test(data_splits)\n",
    "    else:\n",
    "        data_splits = implement_addetive_dataset(data_splits)\n",
    "\n",
    "    return data_splits\n",
    "    \n",
    "\n",
    "\n",
    "def load_incremental_CIFAR20(data_path, remapping= None, uniform_test = False):\n",
    "    trainset, testset, num_channels, _ = load_CIFAR100(data_path)\n",
    "    num_classes = 20\n",
    "    # Split both train and test sets into 20 subsets\n",
    "    train_subsets = split_dataset_into_subsets(trainset, num_classes)\n",
    "    test_subsets = split_dataset_into_subsets(testset, num_classes)\n",
    "\n",
    "    train_subsets = [CIFAR_20_Dataset(subset) for subset in train_subsets]\n",
    "    test_subsets = [CIFAR_20_Dataset(subset) for subset in test_subsets]\n",
    "\n",
    "\n",
    "    # Combine the train and test subsets along with num_channels and num_classes into a list of tuples\n",
    "    data_splits = [(train_subsets[i], test_subsets[i], num_channels, num_classes) for i in range(len(train_subsets))]\n",
    "\n",
    "    data_splits = mix_subsets(data_splits)\n",
    "    if uniform_test:\n",
    "        data_splits = implement_combined_uniform_test(data_splits)\n",
    "    else:\n",
    "        data_splits = implement_addetive_dataset(data_splits)\n",
    "\n",
    "\n",
    "    if remapping is not None:\n",
    "        data_splits = combine_subsets(data_splits, remapping)\n",
    "\n",
    "    return data_splits\n",
    "\n",
    "def split_dataset_into_subsets(dataset, num_subsets=10):\n",
    "    # without assuming the number of classes, grouped into 'num_subsets' subsets\n",
    "    class_groups = {k: [] for k in range(num_subsets)}  # Dict to hold subsets\n",
    "\n",
    "    # Iterate through the dataset to group indices by class\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        # group_key = label // (100 // num_subsets)  # Determine the subset group\n",
    "        group_key = label % num_subsets  # Group by modulo 'num_subsets' of the label\n",
    "        class_groups[group_key].append(idx)\n",
    "\n",
    "    # Create a subset for each group\n",
    "    subsets = [Subset(dataset, indices) for indices in class_groups.values()]\n",
    "    return subsets\n",
    "\n",
    "def remap_dataset(audit_mode, trainset, testset,  train_percent = 0.35, test_percent = 0.35 , audit_percent = 0.3, preserve_original_propertion = True):\n",
    "    \"\"\"\n",
    "    Remaps the given train and test datasets based on the provided percentages. Holds out a portion of the training set for auditing purposes. \n",
    "    Depending on the wheather the audit_mode flag is set, the train and test sets are returned in different ways.\n",
    "\n",
    "    Args:\n",
    "    - trainset: The training dataset to be remapped.\n",
    "    - testset: The testing dataset to be remapped.\n",
    "    - train_percent: The percentage of samples to allocate to the training set (default is 0.35).\n",
    "    - test_percent: The percentage of samples to allocate to the testing set (default is 0.35).\n",
    "    - audit_percent: The percentage of samples to allocate to the audit set (default is 0.3).\n",
    "    - preserve_original_propertion: A boolean indicating whether to preserve the original proportion of the training set overwriting the  train and test percentages (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - train_set: The remapped training dataset.\n",
    "    - test_set: The remapped testing dataset.\n",
    "    \"\"\"\n",
    "    # Concatenate train and test sets\n",
    "    full_dataset = ConcatDataset([trainset, testset])\n",
    "\n",
    "    if preserve_original_propertion:        \n",
    "        original_train_percentage = len(trainset) /len(full_dataset)\n",
    "        train_total_percentage  = 1 - audit_percent        \n",
    "        train_percent = original_train_percentage * train_total_percentage\n",
    "        test_percent = (1-original_train_percentage) * train_total_percentage\n",
    "\n",
    "    # Determine sizes of subsets\n",
    "    num_samples = len(full_dataset)\n",
    "\n",
    "    audit_train_percent = audit_percent *0.6  \n",
    "    \n",
    "    train_size = int(num_samples * train_percent)\n",
    "    test_size = int(num_samples * test_percent)   \n",
    "    audit_train_size = int(num_samples * audit_train_percent)\n",
    "    audit_test_size = num_samples - (audit_train_size + train_size + test_size)       \n",
    "        \n",
    "\n",
    "    # Split the concatenated dataset into subsets\n",
    "    train_set, test_set, audit_train_set, audit_test_set = random_split(full_dataset, [train_size, test_size, audit_train_size, audit_test_size], torch.Generator().manual_seed(42) )\n",
    "    \n",
    "        \n",
    "    \n",
    "    if audit_mode:\n",
    "        train_set = audit_train_set\n",
    "        test_set = audit_test_set\n",
    "    return train_set, test_set\n",
    "\n",
    "def combine_subsets(data_splits, subsets_groups):\n",
    "    new_data_splits = []\n",
    "    for group in subsets_groups:\n",
    "        if isinstance(group, list):  # Group is a list of indices to combine\n",
    "            train_datasets = [data_splits[i][0] for i in group]\n",
    "            test_datasets = [data_splits[i][1] for i in group]\n",
    "            # Assume num_channels and num_classes are consistent within the group\n",
    "            num_channels = data_splits[group[-1]][2]\n",
    "            num_classes = data_splits[group[-1]][3]\n",
    "            combined_train_dataset = ConcatDataset(train_datasets)\n",
    "            combined_test_dataset = ConcatDataset(test_datasets)\n",
    "            new_data_splits.append((combined_train_dataset, combined_test_dataset, num_channels, num_classes))\n",
    "        else:\n",
    "            # Group is a single index, include as is\n",
    "            new_data_splits.append(data_splits[group])\n",
    "    return new_data_splits\n",
    "\n",
    "def implement_addetive_dataset(data_splits, additive_train =False):\n",
    "    new_data_splits = []\n",
    "    expanding_dataset = []\n",
    "    for i, split in tqdm(enumerate(data_splits), leave=False):\n",
    "        train_dataset_i, test_dataset_i, num_channels, num_classes = split\n",
    "        if additive_train:\n",
    "            expanding_dataset.append(train_dataset_i)\n",
    "            split = (ConcatDataset(expanding_dataset), test_dataset_i, num_channels, num_classes)\n",
    "        else:\n",
    "            expanding_dataset.append(test_dataset_i)\n",
    "            split = (train_dataset_i, ConcatDataset(expanding_dataset), num_channels, num_classes)\n",
    "        new_data_splits.append(split)\n",
    "    return new_data_splits\n",
    "\n",
    "def implement_combined_uniform_test(data_splits):    \n",
    "    expanding_dataset = []\n",
    "    for _, split in tqdm(enumerate(data_splits), leave=False):\n",
    "        _, test_dataset_i, _, _ = split\n",
    "        expanding_dataset.append(test_dataset_i)    \n",
    "    combined_uniform_test= ConcatDataset(expanding_dataset)\n",
    "\n",
    "    new_data_splits = []    \n",
    "    for i, split in tqdm(enumerate(data_splits), leave=False):\n",
    "        train_dataset_i, _, num_channels, num_classes = split\n",
    "        split = (train_dataset_i, combined_uniform_test, num_channels, num_classes)\n",
    "        new_data_splits.append(split)\n",
    "    return new_data_splits\n",
    "\n",
    "\n",
    "def get_mixing_proportions(num_classes=20, seed_value=42):\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # Generate a random matrix\n",
    "    matrix = np.random.rand(num_classes, num_classes)\n",
    "\n",
    "    # Normalize each row to sum up to 1\n",
    "    matrix_normalized_row = matrix / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Normalize each column to sum up to 1\n",
    "    matrix_normalized = matrix_normalized_row / matrix_normalized_row.sum(axis=0)\n",
    "\n",
    "    # Round the normalized matrix to 2 decimal places\n",
    "    matrix_rounded = np.around(matrix_normalized, decimals=2)\n",
    "\n",
    "    # Adjust each row to sum to 1, correcting for rounding errors\n",
    "    for i in range(num_classes):\n",
    "        row_diff = 1 - matrix_rounded[i, :].sum()\n",
    "        matrix_rounded[i, np.argmax(matrix_rounded[i, :])] += row_diff\n",
    "\n",
    "    # Adjust each column to sum to 1, correcting for rounding errors\n",
    "    for j in range(num_classes):\n",
    "        col_diff = 1 - matrix_rounded[:, j].sum()\n",
    "        matrix_rounded[np.argmax(matrix_rounded[:, j]), j] += col_diff\n",
    "\n",
    "    return matrix_rounded\n",
    "\n",
    "def mix_subsets(subsets, proportions=None, seed_value=42):\n",
    "    \"\"\"\n",
    "    Mix subsets according to user-defined proportions.\n",
    "\n",
    "    Args:\n",
    "    - subsets: A list of dataset subsets to mix.\n",
    "    - proportions: A list of proportions for each subset.\n",
    "    \n",
    "    Returns:\n",
    "    - A new dataset consisting of mixed subsets.\n",
    "    \"\"\"\n",
    "    num_splits = len(subsets)\n",
    "    if proportions is None:\n",
    "        proportions = get_mixing_proportions(num_splits, seed_value)\n",
    "    else:\n",
    "        assert num_splits == len(proportions)\n",
    "\n",
    "    # Initialize empty lists for the new subsets\n",
    "    new_train_datasets = [[] for _ in range(num_splits)]\n",
    "    new_test_datasets = [[] for _ in range(num_splits)]\n",
    "    new_data_splits = []\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed_value)\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Loop through each original subset\n",
    "        for i, subset in enumerate(subsets):\n",
    "            trainset_i, testset_i, num_channels, num_classes = subset\n",
    "            \n",
    "            # Calculate lengths for the new subsets\n",
    "            lengths_train = [int(p * len(trainset_i)) for p in proportions[i]]\n",
    "            lengths_test = [int(p * len(testset_i)) for p in proportions[i]]\n",
    "            \n",
    "\n",
    "            #fix for rounding errors\n",
    "            lengths_train[-1] = len(trainset_i)-sum(lengths_train[:-1]) \n",
    "            lengths_test[-1] = len(testset_i)-sum(lengths_test[:-1])\n",
    "\n",
    "            # Split the original subsets into new subsets based on the calculated lengths\n",
    "            train_splits = random_split(trainset_i, lengths_train, generator=generator)\n",
    "            test_splits = random_split(testset_i, lengths_test, generator=generator)\n",
    "\n",
    "            # Accumulate the splits into the corresponding new datasets arrays\n",
    "            for i, split in enumerate(train_splits):\n",
    "                new_train_datasets[i].append(split)  \n",
    "            \n",
    "            for i, split in enumerate(test_splits):\n",
    "                new_test_datasets[i].append(split)  \n",
    "\n",
    "        # Now, concatenate the accumulated subsets\n",
    "        for i, (trn_datasets, tst_datasets) in enumerate(zip(new_train_datasets, new_test_datasets)):\n",
    "            split = (ConcatDataset(trn_datasets), ConcatDataset(tst_datasets), num_channels, num_classes)\n",
    "            new_data_splits.append(split)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        pdb.set_trace()\n",
    "\n",
    "    return new_data_splits\n",
    "\n",
    "class CIFAR_20_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.remap = remapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        # # Modify the label to be the new label based on modulo 10\n",
    "        # mod_label = label % 10\n",
    "        coarse_label = self.remap.fine_id_coarse_id[label]\n",
    "        return img, coarse_label\n",
    "\n",
    "class Loss_Label_Dataset(Dataset):\n",
    "    \"\"\"Loss_label_Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dataset, target_model, device, batch_size = 32, loss_batchwise = False):\n",
    "        self.batch_size         = batch_size  \n",
    "        self.loss_batchwise     = loss_batchwise\n",
    "        trainset                = original_dataset[0]\n",
    "        testset                 = original_dataset[1]\n",
    "        seen_count              = trainset.dataset.__len__()\n",
    "        unseen_count            = testset.dataset.__len__()\n",
    "        self.target_model       = target_model\n",
    "        self.device             = device\n",
    "\n",
    "        try:\n",
    "            assert abs(seen_count - unseen_count) < seen_count/10  # roughly ballanced dataset\n",
    "            # print(f'Ballanced dataset: seen {seen_count}, unseen {unseen_count}')\n",
    "        except AssertionError as e:\n",
    "            type  = 'batchwise' if loss_batchwise else 'samplewise'\n",
    "            print(f'\\tUnballanced {type} dataset: seen {seen_count}, unseen {unseen_count}')\n",
    "            # pdb.set_trace()\n",
    "\n",
    "        self.data   = []\n",
    "        self.label  = []\n",
    "\n",
    "        self.append_data_label(trainset, 1.0)\n",
    "        self.append_data_label(testset, 0.0)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        sample = [self.data[idx], self.label[idx]]\n",
    "        return sample\n",
    "    \n",
    "    def append_data_label(self, dataLoader, seen_unseen_label, criterion=None):\n",
    "        if not criterion:\n",
    "            criterion = torch.nn.CrossEntropyLoss( )\n",
    "\n",
    "\n",
    "        for images, labels in dataLoader:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            outputs = self.target_model(images)\n",
    "            if self.loss_batchwise:\n",
    "                loss = criterion(outputs, labels).item()\n",
    "                self.data.append(loss)\n",
    "                self.label.append(seen_unseen_label)               \n",
    "\n",
    "            else:\n",
    "                for i, label in enumerate(labels):\n",
    "                    instance_loss = criterion(outputs[i], label).item()\n",
    "                    self.data.append(instance_loss)\n",
    "                    self.label.append(seen_unseen_label)\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "class Wrapper_Dataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data   = data\n",
    "        self.label  = label\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        sample = [self.data[idx], self.label[idx]]\n",
    "        return sample\n",
    "\n",
    "    \n",
    "class Error_Label_Dataset(Loss_Label_Dataset):\n",
    "    def __init__(self, original_dataset, target_model, device, batch_size=32):\n",
    "        super().__init__(original_dataset, target_model, device, batch_size)\n",
    "\n",
    "    def append_data_label(self, dataLoader, seen_unseen_label, criterion=None):\n",
    "        if not criterion:\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        for images, _ in dataLoader:\n",
    "            images  = images.to(self.device)\n",
    "            outputs = self.target_model(images)           \n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            self.data.append(outputs)\n",
    "            self.label.append(seen_unseen_label)\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "def split_dataloaders(trainset, testset, num_splits: int, split_test = False, val_percent = 10, batch_size=32):#-> tuple[List, List, DataLoader, DataLoader]: \n",
    "    \n",
    "\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    total_size = len(trainset)\n",
    "    partition_size = total_size // num_splits\n",
    "    lengths = [partition_size] * num_splits\n",
    "    lengths[-1] += total_size% num_splits          # adding the reminder to the last partition\n",
    "\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    val_datasets = []\n",
    "    for ds in datasets:\n",
    "        if val_percent == 0:\n",
    "            len_val = 0\n",
    "        else:\n",
    "            len_val = len(ds) // val_percent  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        try:            \n",
    "            trainloaders.append(DataLoader(ds_train, batch_size, shuffle=True))\n",
    "            valloaders.append(DataLoader(ds_val, batch_size))\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            pdb.set_trace()\n",
    "\n",
    "        \n",
    "        val_datasets.append(ds_val)\n",
    "    if split_test:\n",
    "        total_size = len(testset)\n",
    "        partition_size = total_size // num_splits\n",
    "        lengths = [partition_size] * num_splits\n",
    "        lengths[-1] += total_size% num_splits          # adding the reminder to the last partition\n",
    "\n",
    "        datasets = random_split(testset, lengths, torch.Generator().manual_seed(42))\n",
    "        testloaders = []\n",
    "        for ds in datasets:\n",
    "            testloaders.append(DataLoader(ds, batch_size))\n",
    "        unsplit_valloader = None\n",
    "    else: \n",
    "        testloader = DataLoader(testset, batch_size)\n",
    "        unsplit_valloader = DataLoader(torch.utils.data.ConcatDataset(val_datasets), batch_size) #type:ignore\n",
    "\n",
    "    return trainloaders, valloaders, testloader, unsplit_valloader\n",
    "\n",
    "def load_dataloaders(trainset, testset, batch_size=32):\n",
    "    trainloader    = DataLoader(trainset, batch_size, shuffle=True)\n",
    "    testloader     = DataLoader(testset, batch_size)\n",
    "    return trainloader,  testloader\n",
    "\n",
    "def get_dataloaders_subset(dataloader, random_subset_size):\n",
    "    dataset  = dataloader.dataset\n",
    "    lengths  = [random_subset_size, len(dataset) - random_subset_size]\n",
    "    truncated_dataset = random_split(dataset, lengths, torch.Generator().manual_seed(42))\n",
    "    return DataLoader(truncated_dataset[0], dataloader.batch_size, shuffle=True)\n",
    "\n",
    "def merge_dataloaders(trainloaders):    \n",
    "    trn_datasets = []\n",
    "    for loader in trainloaders:\n",
    "        trn_datasets.append(loader.dataset)\n",
    "    return DataLoader(ConcatDataset(trn_datasets), trainloaders[0].batch_size)\n",
    "\n",
    "\n",
    "def load_partitioned_datasets(num_clients: int, dataset_name = 'CIFAR10', data_path=\"~/dataset\", val_percent = 10, batch_size=32, split=None):\n",
    "    if split is None:\n",
    "        dataset = DatasetWrapper(dataset_name, data_path)\n",
    "        return split_dataloaders(dataset.trainset, dataset.testset, num_clients, split_test=False,val_percent=val_percent, batch_size=batch_size), dataset.num_channels, dataset.num_classes\n",
    "    else:\n",
    "        continous_datasets = IncrementalDatasetWraper(dataset_name, data_path)\n",
    "        dataset = continous_datasets.splits[split]\n",
    "        [train_dataset, test_dataset, num_channels, num_classes] = dataset\n",
    "        return split_dataloaders(train_dataset, test_dataset, num_clients, split_test=False, val_percent=val_percent, batch_size=batch_size), num_channels, num_classes "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
